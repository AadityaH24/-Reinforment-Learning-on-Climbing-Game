

--25th/04
13:37
Add custom config file. give command mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier> to train based on file
Current changes:
    made recording steps 10,000 so results and graph on  tensorboard is in 10k incrments instaed of 50ks
    run id = test2

14:06 Model seemed to get stuck on rock. unsure why it didnt reset(time based episode end doesnt seem to work)
    using config de2.yaml. changed episolon decay schedule to constant
    Tried to ensure time based reset is working. It is. unsure why agent got stuck.
    id = debug1

14:20 start with def2 again 
    id = test4
    model seemed to have learned to just chill on top of own stick
    after sometime

14:50 use default3 yaml but with algorithm ppo->sac
    id = sac1
    unity crashed uh oh
    now using batch size of 256 and a buffer size of 1000000
    new id = sac2
    very slow but it runs

--26th/04
10:42 changed only the AgentScript.cs file
    id = neg1
    Changed reward at every time step to 0.4 * xpos + 06 * ypos - 100

20:31 made new scene that is a copy of current scene and placed agent between two rocks
    running default on this
    agent script for scene 2 is agent Script2
    removed xdir reward
    id = newPpo1
    model didnt move much
    changed max range to 4
    id = newPpo2


28th
10:38 added a goal layer and a goal location at the top of the mountain.
        added on trigger event in agent code for goal reward of 100 and ending episode and deleted some ground on the left
        trigger script is set on the child
        goalPpo1

11:52 ensure the hammer doesnt leave the player and gets stuck
    add hammer and goal distance as observations vector size is 7
    max range =4 goalPpo2

13:20 set epsilon to 0.4, sfchel;udfe is constant. mnum layers = 3
13:40 fix the hammer thing and set shceudle to linrear
    goalPpo5 clamp range of inputs is higher now

3:06 lr= 0.03, ep =0.4, schedule = constant for both

20:08 sac